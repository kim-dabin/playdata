# 개요#1

## HDFS 기본

 - 하나의 HDFS에 하나의 네임스페이스 제공

 - 파일은 블록들로 저장됨
	보통 64MB ~ 128MB
	큰 파일을 다루는데 적합한 시스템
	
 - 하부 운영체제의 파일 시스템을 그대로 사용

 - 복제를 통한 안정성
	각 블록은 3대의 DataNode에 복제
	
 - Write Once Read Many
	File Overwrite not Append
	
 - 마스터가 모든 메타데이터 관리 >> NameNode로 관리한다.
	간단한 중앙 집중 관리
	
 - No 데이터 캐시
	
데이터 사이즈가 커서 캐시로 인한 혜택 없음



### NameNode

 - 파일시스템의 NameSapce관리
	파일명과 블록들의 집합을 관리
	블록과 블록이 위치한 데이터 노드들의 정보 관리
	블록에 대한 복제 스케줄링
 - 메타데이터 >> hadoop01에서 /home/hadoop/hadoop-2.8.5/mydata/name/current에서 확인할 것 >> 여기에 있는것을 메타 데이터라고 한다.
	FSImage	: 네임스페이스/Inode 정보
	 - 파일들의 리스트, 파일 별 속성 정보
	 - 각 블록 별 DataNode들의 리스트
	  Edit Log	: 트랜잭션 로그
	 - 파일 생성, 삭제 기록들



### Secondary NameNode

 - NameNode의 FSimage와 트랜잭션 로그를 복사함

 - 복사한 FSimage와 트랜잭션 로그(Edit log) 병합

 - 새로운 FSimage를 NameNode에게 보냄

 - Secondary NameNode >> 이게 없다면 데이터 노드들은 블럭이미지를 가지고 관리를 하므로 os자체가 우선순위를 가려버린다. >> 명시해주는걸 가져다 쓰고 싶으므로 세컨더리를 설정한다.
	NameNode의 단순 백업 데몬이 아님
	
FSimage관리를 공동 담당



### DataNode

 - 블록서버
	블록을 메타데이터를 로컬에 저장(ext3)
	클라이언트에게 블록 & 메타데이터 제공
	
 - 블록 보고
	주기적으로(Heart Beat) 존재하는 모든 블록들의 리스트를 NameNode에 보고
	기본 3초마다 (dfs.heartbeat.interval in hdfs-site.xml) >> dfs.namenode.heartbeat.recheck-interval 생겼다.
	
 - Pipelining 데이터 저장
	데디터를 다른 DataNode에 전달
	
 - 블록 배치 순서 >> 128MB로 분할을 해서 메타데이터를 복제해서 동일하게 가지고 있다.
	첫 번째 복제본은 로컬 노드에 저장
	두 번째 복제본은 원격 Rack에 저장
	세 번째 복제본은 같은 Rack에 저장
	
 - 클라이언트 읽기 >> 어디서 출발을 하던 똑같다.
	
로컬노드 -> 같은 Rack -> 원격 Rack 순서

	[HDFS 아키텍처]
	
 - 블록 구조 파일 시스템
	블록 사이즈가 기본적으로 64MB로 설정되어 있음(설정 파일에서 수정 가능)
	파일 사이즈가 기본 블록 설정보다 작을 경우 파일 사이즈로 저장
	블록을 저장할 때 기본적을 3개씩 블록의 복제본을 저장(수정 가능)
	
	>> 지금 우리꺼에서는 repl도 3이고 datanode도 3이다.
	
 - 네임노드와 데이터노드
	
데이터노드에 문제가 생기면 네임노드가 막아버린다.

	>> [데이터 읽기]
	
 - HDFS에서 읽기 >> 블럭리더기가 반드시 존재한다.
	a. HDFS 클라이언트가 네임노드에게 파일이름을 넘기면
	b. 파일의 데이터 블럭 위치 리스트를 넘김
	c. 데이터 노드에게 직접 블럭별 요청 (TCP 50010포트)
	
 - Check Sum 저장
	클라이언트는 블록의 Check Sum 계산
	DataNode는 블록과 Check Sum 저장 >> 얼마나 동일한 데이터를 접근하거나 가져왔는지를 저장
	
 - 데이터 무결성 검사
	
데이터 무결성 검사(Check Sum) 실패시 다른 블록 읽기를 시도

	[IO성능과 확장성]
	
- 쓰기보다는 읽기, 작은 파 일보다는 큰 파일에 최적

    [DataNode 장애 복구]

    >> 사진
    >> Send heartbeat message(10sec)인게 중요한 포인트야
    >> 10초가 최적이다.

 - NameNode가 다시 복구될 경우
	Secondary NameNode는 NameNode의 가장 최신 정보로 재설정
	
 - 원래 NameNode가 복구가 안될 경우
	Secondary Namenode의 메타데이터를 복사한 후 모든 클러스터를 재시작해야함 >> 그렇지 않을경우 데이터가 오지 않는다.
	
DNS를 활용하여 재시작은 피할 수 있음

	[보안모델 & Balancer]
	
 - 보안 모델
	ID기반의 Access Mode 제어
		사용자 ID별 Access Mode설정
		1.x 부터는 Kerberos 인증 방식 적용
	IP 기반의 접근 제어
		시스템에 접속 할수 있는 IP 관리
	
 - Balancer >> 우리는 같은 걸 복제를 해서 복사를 했기 때문에 균등하지만 기기가 다르고 크기가 다를 경우 이걸 해줘서 균등하게 분배해줘야한다. >> 경로만 맞으면 인식을 하기에 클러스터 내 재분배가 필요하다
		>> 균등하게 분배하지 않으면 시간차가 발생할 수 있다.
	>> 클러스터 내 데이터 블록을 균등하게 재분배
	>> 'dfs.datanode.du.reserved' 옵션과 함께 사용해서 데이터 분배를 제어
>> =======================================================================================================================
>> [POSIX]
		
 - MountableHDFS
	HDFS의 Contribute 프로젝트
	FUSE를 이용하여 POSIX 지원
	현재는 Native 대비 약 20% 성능 저하
	
 - Quata
	NameNode의 확장성에 대한 보완
		NameNode 메모리에 의해서 저장 가능한 파일 개수에 한계가 있으며 초과시에는 장애 상황
	
디렉토리 별 파일의 개수와 총 저장 양을 제한하는 기능
>> 여기는 설명 아니고 어제 한거에서 이어서!!!
27. FUSE POSIX
os에 객체 단위 또는 파일단위로 HDFS에 파일 탐색을 하게 되면 리눅스는 커널을 통해 마운트라는 작업을 제공하게 된다.
FUSE라는 것 자체는 리눅스 커널을 통해 마운트할 수 있게 해주는 것이다.
 >> 리눅스 모듈을 그대로 적용해서 사용한다.

28. core-site.xml >> temp 폴더 다 만들기
  <property>
    <name>dfs.data.dir</name>
    <value>/home/hadoop/hadoop-2.8.5/temp</value>
  </property>


[hadoop@hadoop02 ~]$ hadoop fs -put -d /home/hadoop/hadoop-2.8.5/NOTICE.txt /test
[hadoop@hadoop02 ~]$ hadoop fs -ls /test
-rw-r--r--   3 hadoop supergroup      15915 2020-09-16 11:17 /test/NOTICE.txt
-rw-r--r--   3 hadoop supergroup          0 2020-09-16 09:56 /test/README.txt
>> hadoop02에서 만들어도 두개 다 공유해서 보인다.

29. hdfs에 파일 권한 설정 변경
[hadoop@hadoop02 ~]$ hadoop fs -chmod -R 777 /test/README.txt
[hadoop@hadoop02 ~]$ hadoop fs -ls /test
-rw-r--r--   3 hadoop supergroup      15915 2020-09-16 11:17 /test/NOTICE.txt
-rwxrwxrwx   3 hadoop supergroup          0 2020-09-16 09:56 /test/README.txt

30. 휴지통만들기 >> core-site.xml
  <property>
    <name>fs.trash.interval</name>
    <value>1000</value>
  </property>
scp -r /home/hadoop/hadoop-2.8.5/etc/hadoop/core-site.xml hadoop@hadoop02:/home/hadoop/hadoop-2.8.5/etc/hadoop/core-site.xml
scp -r /home/hadoop/hadoop-2.8.5/etc/hadoop/core-site.xml hadoop@hadoop03:/home/hadoop/hadoop-2.8.5/etc/hadoop/core-site.xml
scp -r /home/hadoop/hadoop-2.8.5/etc/hadoop/core-site.xml hadoop@hadoop04:/home/hadoop/hadoop-2.8.5/etc/hadoop/core-site.xml
stop-all.sh
start-all.sh
>> https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge
>> https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes
>> expunge 명령 실행해보고 .Trash가 어디있는지 확인할 것

31. eclipse에서 jar파일 만들기 >> api 예제를 실행
	1) window에서 WordCount.jar를 공용폴더에 저장한다.
	2) /home/hadoop으로 옮겨서 권한을 변경한다.
		[root@hadoop01 sf_mywork]# mv WordCount.jar /home/hadoop
		[root@hadoop01 hadoop]# chown hadoop:hadoop WordCount.jar
		[root@hadoop01 hadoop]# su hadoop
	3) file01 / file02파일을 두개 생성한다.
	
	```bash
		[hadoop@hadoop01 ~]$ cat > file01
		Hello World Bye World
		[hadoop@hadoop01 ~]$ cat > file02
		Hello Hadoop Goodbye Hadoop
	```
	
	4) hdfs로 디렉토리 생성 후 파일 업로드
	
	```bash
	[hadoop@hadoop01 ~]$ hadoop fs -mkdir -p /user/joe/wordcount/input/
	[hadoop@hadoop01 ~]$ hadoop fs -put file01 /user/joe/wordcount/input/
	[hadoop@hadoop01 ~]$ hadoop fs -put file02 /user/joe/wordcount/input/
	
		
```

	5) 올린 파일 목록과 내용 확인 >> 어디서 하든 실행이 되어야한다.
	
	```bash
	[hadoop@hadoop02 ~]$ hadoop fs -ls /user/joe/wordcount/input/
	[hadoop@hadoop02 ~]$ hadoop fs -cat /user/joe/wordcount/input/file01
	[hadoop@hadoop02 ~]$ hadoop fs -cat /user/joe/wordcount/input/file02
	```
	
	6) 프로그램 실행 파일로 파일을 업로드한 결과를 집계
	
	```bash
	$ hadoop jar myWordCount.jar com.test.WordCount /user/joe/wordcount/input /user/joe/wordcount/output
	```
	
	7) 집계낸 결과를 확인
	
	```bash
	$ hadoop fs -cat /user/joe/wordcount/output/part-r-00000
	```
	
	<img src="../../../Library/Application Support/typora-user-images/image-20200917122708678.png" alt="image-20200917122708678" style="zoom:50%;" />
	
	
	
	HDFS 페러데이션
 - HDFS가 네임노드에 기능이 집중되는 것을 막기 위함
 - 기존 HDFS 문제점
	네임노드 확장
		수평정 확장 불가능
		네임노드 구동시 전체 파일 시스템 이미지를 메모리에 생성
		 - 64GB RAM 약 2억 5천 만개 파일과 블록 저장 가능
	성능
		HDFS에 대한 모든 요청이 네임노드에 요청됨
		쓰기시 트랜잭션에 대한 에디트 로그 생성
	동일 네임노드 사용
	네임스페이스와 블록 관리의 지나친 결합
 - HDFS 페러데이션의 장점
	네임노드 분리
	기존 구성 변경 불필요
	네임노드 등록 삭제시 재시작 불필요
	밸런서 디터미션 네임노드 개별적 수행 가능
=======================================================================================================================
[YARN]
 - YARN 등장 배경
	잡 트래커가 모든 잡의 스케줄링과 자원관리
	잡 트래커 구조 확장의 필요성
	메모리 소비 개선
	스레드 모델 개선 등등
 - YARN (Yet Another Resource Negotiator)
	자원 관리, Job 상태 관리를 ResourceManager와 ApplicationMaster로 분리하여, 기존 Job Tracker에 몰리던 병목을 제거
	MapReduce외에 다양한 어플리케이션을 실행할 수 있으며, 어플리케이션마다 자원(CPU, 메모리)을 할당받음
		클러스터에는 여러개의 Application이 동작가능
		각 Application은 ApplicationMaster가 모든 task를 관리
 - 자원할당의 유연성
	HADOOP 1
		Slot에 미리 자원(memory, cpu cores)를 할당 후 미리 정해진 설정에 따라서 slot을 job에 할당
		Job이 모두 끝나기 전까지는 자원이 반납되지 않는 문제
	HADOOP 2 YARN
		요청이 있을 때마다 요구하는 자원의 spec에 맞게 자원을 container의 개념으로 할당
		container마다 다른 spec의 자원을 갖을 수 있음
		모든 task는 container에서 수행되고 task가 끝나는 즉시 자원을 반납
 - 리소스 매니저(Resource Manager)
	master node에서 동작
	global resource scheduler
	application들의 자원 요구의 할당 및 관리
 - 노드 매니저(Node Manager)
	slave node에서 동작
	node의 자원을 관리
	container에 node의 자원을 할당
 - 컨테이너(Container)
	RM(Resource Manager)의 요청에 의해 NM(Node Manager)에서 할당
	slave node의 CPU core, memory의 자원을 할당
	aplications은 다수의 containser로 동작
=======================================================================================================================
[MapReduce]
 - HDFS에 분산 저장된 데이터에 스트리밍 접근을 요청하며 빠르게 분산 처리하도록 고안된 프로그래밍 모델, 이를 지원하는 시스템
 - 대규모 분산 컴퓨팅 혹은 단일 컴퓨팅 환경에서 개발자가 대량의 데이터를 명렬로 분석할 수 있음
 - 개발자는 맵리듀스 알고리즘에 맞게 분석 프로그램을 개발하고, 데이터의 입출력과 병렬 처리등 기반 작업은 프레임워크가 알아서 처리해줌
 - 맵(Map) : (k1, v1) -> list(k2, v2)
	데이터를 가공해서 분류 (연산 가공자)
 - 리듀스(Reduce) : (k2, list(v2)) -> list(k3, v3)
	
분류된 데이터를 통합(집계 연산자)

	[Combiner]
 - 셔플(Shuffle)
	맵 태스크와 리듀스 태스크 사이의 데이터 전달 과정
	맵 태스크의 출력 데이터는 네트워크를 통해 리듀스 태스크로 전달됨
 - 콤바이너 클래스
	셔플할 데이터의 크기를 줄이는데 도움을 줌
		매퍼의 출력 데이터를 입력데이터로 전달받아 연산을 수행
	
로컬 노드에서 로컬에 생성된 매퍼의 출력 데이터를 이용하기 때문에 네트워크 비용이 발생하지 않음





